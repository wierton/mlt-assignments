\documentclass[a4paper,UTF8]{article}
\usepackage{ctex}
\usepackage[margin=1.25in]{geometry}
\usepackage{color}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{bm}
\usepackage{tcolorbox}
\usepackage{hyperref}
\usepackage{pgfplots}
\usepackage{epsfig}
\usepackage{color}
\usepackage{mdframed}
\usepackage{lipsum}
% add the new command "\norm"
\usepackage{mathtools}
\let\oldnorm\norm   % <-- Store original \norm as \oldnorm
\let\norm\undefined % <-- "Undefine" \norm
\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\newmdtheoremenv{thm-box}{myThm}
\newmdtheoremenv{prop-box}{Proposition}
\newmdtheoremenv{def-box}{定义}

\setlength{\evensidemargin}{.25in}
\setlength{\textwidth}{6in}
\setlength{\topmargin}{-0.5in}
\setlength{\topmargin}{-0.5in}
% \setlength{\textheight}{9.5in}
%%%%%%%%%%%%%%%%%%此处用于设置页眉页脚%%%%%%%%%%%%%%%%%%
\usepackage{fancyhdr}                                
\usepackage{lastpage}                                           
\usepackage{layout}                                             
\footskip = 10pt 
\pagestyle{fancy}                    % 设置页眉                 
\lhead{2019年春季}                    
\chead{机器学习理论导引}                                                
% \rhead{第\thepage/\pageref{LastPage}页} 
\rhead{作业四}                                                                                               
\cfoot{\thepage}                                                
\renewcommand{\headrulewidth}{1pt}  			%页眉线宽，设为0可以去页眉线
\setlength{\skip\footins}{0.5cm}    			%脚注与正文的距离           
\renewcommand{\footrulewidth}{0pt}  			%页脚线宽，设为0可以去页脚线

\makeatletter 									%设置双线页眉                                        
\def\headrule{{\if@fancyplain\let\headrulewidth\plainheadrulewidth\fi%
		\hrule\@height 1.0pt \@width\headwidth\vskip1pt	%上面线为1pt粗  
		\hrule\@height 0.5pt\@width\headwidth  			%下面0.5pt粗            
		\vskip-2\headrulewidth\vskip-1pt}      			%两条线的距离1pt        
	\vspace{6mm}}     								%双线与下面正文之间的垂直间距              
\makeatother  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\numberwithin{equation}{section}
%\usepackage[thmmarks, amsmath, thref]{ntheorem}
\newtheorem{theorem}{Theorem}
\newtheorem*{myDef}{Definition}
\newtheorem*{mySol}{Solution}
\newtheorem*{myAnalysis}{Analysis}
\newtheorem*{myProof}{Proof}
\newtheorem*{myRemark}{备注}

\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}

\usepackage{multirow}

%--

%--
\begin{document}
\renewcommand{\tilde}{\mathbf{w}idetilde}
\renewcommand{\hat}{\mathbf{w}idehat}
	\title{机器学习理论导引\\
		作业四}
	\author{DZ1833019,欧先飞,ouxianfei@smail.nju.edu.cn}
	\maketitle
	
	% \section*{作业提交注意事项}
	% \begin{tcolorbox}
	% 	\begin{enumerate}
	% 		\item[(1)] 请严格参照课程网站作业提交方法一节提交作业，未按照要求提交作业，或提交作业格式不正确，将会被扣除部分作业分数；
	% 		\item[(2)] 截止时间后不接收作业，本次作业记零分；
	% 		\item[(3)] 本课程要求大家遵守学术诚信，请关注课程主页的详细说明。
	% 	\end{enumerate}
	% \end{tcolorbox}
	
	% \newpage

\section{[15pts] Conjugate Functions}
\noindent 推导下面函数的共轭函数
\[
	f(x)=\log(1+\exp(-x)).
\]
	
\begin{myProof}~\\

首先由定义有：
\begin{eqnarray*}
	f^*(y) = \sup_{y \in dom(f^*)} (yx - f(x)) = \sup_{y \in dom(f^*)} (yx - \ln(1 + e^{-x}))
\end{eqnarray*}

令$g(x) = yx - \ln(1 + e^{-x})$，对其求导则有$g'(x) = y + \frac{e^{-x}}{1 + e^{-x}} = y + \frac{1}{1 + e^x}$，容易发现在整个定义域内，$g'(x)$呈上升趋势且$g'(x) > y$恒成立，以下对$y$的取值范围进行讨论。

$(1)$  当$y > 0$时，$g(x)$在$(-\infty, +\infty)$内单调递增，则$f^*(y) = \sup g(x) = \lim_{x \rightarrow +\infty} (yx - \ln(1 + e^{-x})) = +\infty$，此时$f^*(y)$无定义。

$(2)$ 当$y = 0$时，$g(x)$在$(-\infty, +\infty)$内单调递增，则$f^*(y) = \sup g(x) = \lim_{x \rightarrow +\infty} (yx - \ln(1 + e^{-x})) = 0$。

$(3)$ 当$-1 < y < 0$时，$g'(x)$在定义域内存在零点$\ln(-1 - \frac{1}{y})$，亦即$g(x)$在$x=\ln(-1 - \frac{1}{y})$时达到最值，故$f^*(y) = \sup g(x) = g(\ln(-1 - \frac{1}{y})) = y \ln(-1 - \frac{1}{y}) - \ln \left( 1 + e^{-\ln(-1 - \frac{1}{y})} \right) = y \ln(-1 - \frac{1}{y}) - \ln \left( \frac{1}{y+1} \right)$。

$(4)$ 当$y = -1$时，$g'(x) < 0$在$(-\infty, +\infty)$内恒成立，所以$\sup g(x) = \lim_{x \rightarrow -\infty} ( yx - \ln(1 + e^{-x}) ) = \lim_{x \rightarrow -\infty} \ln \frac{e^{(y+1)x}}{1+e^x} = 0$。

$(5)$ 当$y < -1$时，$\lim_{x \rightarrow -\infty} ( yx - \ln(1 + e^{-x}) ) = +\infty$，所以$\sup g(x)$不存在，此时$f^*(y)$无定义。

综上所述
\begin{eqnarray*}
f^*(y) = \left\{
\begin{aligned}
	0 &,& y = 0 \\
	y \ln(-1 - \frac{1}{y}) - \ln \left( \frac{1}{y+1} \right) &,& -1 < y < 0 \\
	0 &,& y = -1 \\
\end{aligned}
\right.
\end{eqnarray*}

	~\\	
	\qed
\end{myProof}

\newpage
\section{[15pts] Projection}
\noindent 对于凸集$\mathcal{W}$，试证明投影操作$\Pi_{\mathcal{W}}(\cdot)$是不扩展的，即
\[
\norm{\Pi_{\mathcal{W}}(\mathbf{x})-\Pi_{\mathcal{W}}(\mathbf{y})} \leq \norm{\mathbf{x} - \mathbf{y}}, \ \forall \mathbf{x}, \mathbf{y}.
\]

\begin{myProof}~\\
首先简记$\Pi_{\mathcal{W}}(\mathbf{x})$为$\mathbf{x}_\mathcal{W}$，由于$\mathcal{W}$是凸包，所以有$\forall \mathbf{x} \notin \mathcal{W} \forall \mathbf{y} \in \mathcal{W}  (\mathbf{y} - \mathbf{x}_\mathcal{W})^\mathrm{T}(\mathbf{x}-\mathbf{x}_\mathcal{W}) \le 0 $，以下对$\mathbf{x}$和$\mathbf{y}$的所处的位置进行讨论。
~\\
$(1)$ $\mathbf{x}$和$\mathbf{y}$均在凸包$\mathcal{W}$范围内，则$\mathbf{x}_\mathcal{W} = \mathbf{x}$、$\mathbf{y}_\mathcal{W} = \mathbf{y}$，所以有 $\norm{\mathbf{x}_\mathcal{W}-\mathbf{y}_\mathcal{W}} = \norm{\mathbf{x} - \mathbf{y}}$
~\\
$(2)$ 两点中有且只有一点在$\mathcal{W}$中，不妨设$\mathbf{y}$在$\mathcal{W}$中。反设原命题不成立，即有$\norm{\mathbf{x} - \mathbf{y}} < \norm{\mathbf{x}_\mathcal{W} - \mathbf{y}_\mathcal{W}} = \norm{\mathbf{x}_\mathcal{W} - \mathbf{y}}$，于是有

\begin{gather}
\norm{\mathbf{x} - \mathbf{y}}_2^2 < \norm{\mathbf{x}_\mathcal{W} - \mathbf{y}}_2^2 \label{eq:r1} \\
(\mathbf{y}-\mathbf{x}_\mathcal{W})^\mathrm{T}(\mathbf{x} - \mathbf{x}_\mathcal{W}) \le 0  \label{eq:r2}
\end{gather}

计算$\ref{eq:r1} + 2 \times \ref{eq:r2}$则有：
\begin{eqnarray*}
&& (\mathbf{x}-\mathbf{y})^\mathrm{T}(\mathbf{x}-\mathbf{y}) + 2(\mathbf{y} - \mathbf{x}_\mathcal{W})^\mathrm{T}(\mathbf{x}-\mathbf{x}_\mathcal{W}) < (\mathbf{x}_\mathcal{W} - \mathbf{y})^\mathrm{T}(\mathbf{x}_\mathcal{W} - \mathbf{y}) \\
&\Longrightarrow& 
(\mathbf{x}-\mathbf{y})^\mathrm{T}(\mathbf{x}-\mathbf{y}) + (\mathbf{y} - \mathbf{x}_\mathcal{W})^\mathrm{T}(2\mathbf{x} - 2\mathbf{x}_w - \mathbf{y} + \mathbf{x}_\mathcal{W}) < 0 \\
&\Longrightarrow&
(\mathbf{x}-\mathbf{y})^\mathrm{T}(\mathbf{x}-\mathbf{y}) + (\mathbf{y} - \mathbf{x}_\mathcal{W})^\mathrm{T}(\mathbf{x} - \mathbf{y} + \mathbf{x} - \mathbf{x}_\mathcal{W}) < 0 \\
&\Longrightarrow&
(\mathbf{x}-\mathbf{y})^\mathrm{T}(\mathbf{x}-\mathbf{y}) + (\mathbf{y} - \mathbf{x}_\mathcal{W})^\mathrm{T}(\mathbf{x} - \mathbf{y}) + (\mathbf{y} - \mathbf{x}_\mathcal{W})^\mathrm{T}(\mathbf{x} - \mathbf{x}_\mathcal{W}) < 0 \\
&\Longrightarrow&
(\mathbf{x} - \mathbf{x}_\mathcal{W})^\mathrm{T}(\mathbf{x} - \mathbf{y}) + (\mathbf{y} - \mathbf{x}_\mathcal{W})^\mathrm{T}(\mathbf{x} - \mathbf{x}_\mathcal{W}) < 0 \\
&\Longrightarrow&
(\mathbf{x} - \mathbf{x}_\mathcal{W})^\mathrm{T}(\mathbf{x} - \mathbf{x}_\mathcal{W})^\mathrm{T} < 0 \\
&\Longrightarrow&
\norm{\mathbf{x} - \mathbf{x}_\mathcal{W}}_2^2 < 0
\end{eqnarray*}

这与事实$\norm{\mathbf{x} - \mathbf{x}_\mathcal{W}}_2^2 \ge 0$矛盾，所以反设不成立，继而有$\norm{\mathbf{x} - \mathbf{y}} \ge \norm{\mathbf{x}_\mathcal{W} - \mathbf{y}}$。
~\\
$(3)$两点均在$\mathcal{\mathcal{W}}$外，由凸包的性质，可以得到如下几个条件：
\begin{gather}
(\mathbf{y}_\mathcal{W} - \mathbf{x}_\mathcal{W})^\mathrm{T}(\mathbf{x} - \mathbf{x}_\mathcal{W}) \le 0 \label{eq:z1} \\
(\mathbf{x}_\mathcal{W} - \mathbf{y}_\mathcal{W})^\mathrm{T}(\mathbf{y} - \mathbf{y}_\mathcal{W}) \le 0 \label{eq:z2}
\end{gather}

计算$\ref{eq:z1} + \ref{eq:z2}$可以得到：$(\mathbf{x}_\mathcal{W} - \mathbf{y}_\mathcal{W})^\mathrm{T}(\mathbf{x}_\mathcal{W} - \mathbf{y}_\mathcal{W}) < (\mathbf{x}_\mathcal{W} - \mathbf{y}_\mathcal{W})^\mathrm{T}(\mathbf{x} - \mathbf{y})$，而$(\mathbf{x}_\mathcal{W} - \mathbf{y}_\mathcal{W})^\mathrm{T}(\mathbf{x} - \mathbf{y}) = \norm{\mathbf{x}_\mathcal{W} - \mathbf{y}_\mathcal{W}}\norm{\mathbf{x}-\mathbf{y}} \cos \langle \mathbf{x}_\mathcal{W} - \mathbf{y}_\mathcal{W}, \mathbf{x} - \mathbf{y} \rangle \le \norm{\mathbf{x}_\mathcal{W} - \mathbf{y}_\mathcal{W}}\norm{\mathbf{x} - \mathbf{y}}$，所以有$\norm{\mathbf{x}_\mathcal{W} - \mathbf{y}_\mathcal{W}}_2^2 < \norm {\mathbf{x}_\mathcal{W} - \mathbf{y}_\mathcal{W}}\norm{\mathbf{x} - \mathbf{y}}$，由于二范数的非负性，所以可以化简得到$\norm{\mathbf{x}_\mathcal{W} - \mathbf{y}_\mathcal{W}} \le \norm{\mathbf{x} - \mathbf{y}}$。

~\\
结合$(1)$、$(2)$、$(3)$可知，命题$\norm{\mathbf{x}_\mathcal{W} - \mathbf{y}_\mathcal{W}} \le \norm{\mathbf{x} - \mathbf{y}}$成立。


~\\

	\qed
\end{myProof}

\newpage
\section{[20pts] Gradient Descent with Decaying Step Size}
\noindent 分析采用衰减步长时梯度下降（GD）的收敛速率。具体而言，考虑
\[
\eta_t=O\left(\frac{1}{\sqrt{t}}\right)
\]

\begin{myAnalysis}~\\
设最优值为$\mathbf{w}$，首先参考slides中的证明有：
\begin{eqnarray*}
f(\mathbf{w}_t) - f(\mathbf{w}) &\le& \nabla f(\mathbf{w}_t) ^ T (\mathbf{w}_t - \mathbf{w}) \\
&=& \frac{1}{\eta_t} \langle \mathbf{w}_t - \mathbf{w}_{t+1}', \mathbf{w}_t - \mathbf{w} \rangle \\
&=& \frac{1}{2\eta_t} \left[ \norm{\mathbf{w}_t - \mathbf{w}_{t+1}'}_2^2 + \norm{\mathbf{w}_t - \mathbf{w}}_2^2 - \norm{\mathbf{w}_{t+1}' - \mathbf{w}}_2^2 \right] \\
&=& \frac{1}{2\eta_t} \left[ \norm{\mathbf{w}_t - \mathbf{w}}_2^2 - \norm{\mathbf{w}_{t+1}' - \mathbf{w}}_2^2 \right] + \frac{1}{2\eta_t} \norm{\mathbf{w}_t - \mathbf{w}_{t+1}'}_2^2 \\
&=& \frac{1}{2\eta_t} \left[ \norm{\mathbf{w}_t - \mathbf{w}}_2^2 - \norm{\mathbf{w}_{t+1}' - \mathbf{w}}_2^2 \right] + \frac{\eta_t}{2} \norm{\nabla f(\mathbf{w}_t)}_2^2 \\
&\le& \frac{1}{2\eta_t} \left[ \norm{\mathbf{w}_t - \mathbf{w}}_2^2 - \norm{\mathbf{w}_{t+1} - \mathbf{w}}_2^2 \right] + \frac{\eta_t}{2} G^2
\end{eqnarray*}

将上面得到的不等式从$t=1$加到T则有：
\begin{eqnarray*}
\sum_{t=1}^T \left[ f(\mathbf{w}_t) - f(\mathbf{w}) \right] &\le& \sum_{t=1}^T \frac{1}{2\eta_t} \norm{\mathbf{w}_t - \mathbf{w}}_2^2 - \sum_{t=1}^T \frac{1}{2\eta_t} \norm{\mathbf{w}_{t+1} - \mathbf{w}}_2^2 + \frac{1}{2} \sum_{t=1}^T \eta_t G^2 \\
&=& \frac{1}{2\eta_1} \norm{\mathbf{w}_1 - \mathbf{w}}_2^2 + \sum_{t=1}^{T - 1} \left( \frac{1}{2\eta_{t+1}} - \frac{1}{2\eta_t} \right) \norm{\mathbf{w}_{t+1} - \mathbf{w}}_2^2 - \frac{1}{2\eta_T}\norm{\mathbf{w}_{T+1} - \mathbf{w}}_2^2 + \frac{1}{2} \sum_{t=1}^T \eta_t G^2
\end{eqnarray*}

由题意可设$\eta_t = \frac{a}{\sqrt{t}}$，而$\sum_{t=1}^T \frac{1}{\sqrt{t}} < 1 + \int_1^T \frac{1}{\sqrt{x}} dx = 1 + (2\sqrt{x})|_1^T = 2\sqrt{T} - 1$，代入上面的不等式则有：
\begin{eqnarray*}
\sum_{t=1}^T \left[ f(\mathbf{w}_t) - f(\mathbf{w}) \right] &\le& \frac{1}{2\eta_1} \norm{\mathbf{w}_1 - \mathbf{w}}_2^2 + \sum_{t=1}^{T - 1} \left( \frac{1}{2\eta_{t+1}} - \frac{1}{2\eta_t} \right) \norm{\mathbf{w}_{t+1} - \mathbf{w}}_2^2 - \frac{1}{2\eta_T}\norm{\mathbf{w}_{T+1} - \mathbf{w}}_2^2 + \frac{1}{2} \sum_{t=1}^T \eta_t G^2 \\
&=& \frac{\norm{\mathbf{w}_1 - \mathbf{w}}_2^2}{2a} + \sum_{t=1}^{T - 1} \left( \frac{\sqrt{t+1}}{2a} - \frac{\sqrt{t}}{2a} \right) \norm{\mathbf{w}_{t+1} - \mathbf{w}}_2^2 - \frac{\sqrt{T}}{2a}\norm{\mathbf{w}_{T+1} - \mathbf{w}}_2^2 + \frac{2\sqrt{T} - 1}{2} aG^2 \\
&=& \frac{\norm{\mathbf{w}_1 - \mathbf{w}}_2^2}{2a} + \sum_{t=1}^{T - 1} \frac{1}{2a(\sqrt{t+1} + \sqrt{t})}  \norm{\mathbf{w}_{t+1} - \mathbf{w}}_2^2 - \frac{\sqrt{T}}{2a}\norm{\mathbf{w}_{T+1} - \mathbf{w}}_2^2 + \frac{2\sqrt{T} - 1}{2} aG^2 \\
&<& \frac{\norm{\mathbf{w}_1 - \mathbf{w}}_2^2}{2a} + \sum_{t=1}^{T - 1} \frac{1}{4a\sqrt{t}}  \norm{\mathbf{w}_{t+1} - \mathbf{w}}_2^2 - \frac{\sqrt{T}}{2a}\norm{\mathbf{w}_{T+1} - \mathbf{w}}_2^2 + \frac{2\sqrt{T} - 1}{2} a G^2 \\
&=& \frac{\norm{\mathbf{w}_1 - \mathbf{w}}_2^2}{2a} + \frac{2\sqrt{T-1} -1}{4a}  \norm{\mathbf{w}_{t+1} - \mathbf{w}}_2^2 - \frac{\sqrt{T}}{2a}\norm{\mathbf{w}_{T+1} - \mathbf{w}}_2^2 + \frac{2\sqrt{T} - 1}{2}a G^2
\end{eqnarray*}

参考slides中对定义域直径的假设，上述不等式可以继续化简为：
\begin{eqnarray*}
\sum_{t=1}^T \left[ f(\mathbf{w}_t) - f(\mathbf{w}) \right] &\le& \frac{\norm{\mathbf{w}_1 - \mathbf{w}}_2^2}{2a} + \frac{2\sqrt{T-1} -1}{4a}  \norm{\mathbf{w}_{t+1} - \mathbf{w}}_2^2 - \frac{\sqrt{T}}{2a}\norm{\mathbf{w}_{T+1} - \mathbf{w}}_2^2 + \frac{2\sqrt{T} - 1}{2} aG^2 \\
&\le& \frac{D^2}{2a} + \frac{2\sqrt{T-1} -1}{4a}  D^2 + \frac{2\sqrt{T} - 1}{2}a G^2 \\
&=& \frac{D^2}{2a} \left( 1 + \frac{2\sqrt{T-1} -1}{2} \right) + \frac{2\sqrt{T} - 1}{2} G^2 \\
&<& \frac{D^2}{2a} \left( 1 + \sqrt{T} \right) + \sqrt{T}a G^2
\end{eqnarray*}

再由Jensen不等式便有$f(\overline{\mathbf{w}}_T) - f(\mathbf{w}) < \left( \frac{1}{T} \sum_{t=1}^T f(\mathbf{w}_t) \right) - f(\mathbf{w}) < \frac{D^2}{2a} (\frac{1}{T} + \frac{1}{\sqrt{T}}) + \frac{G^2}{\sqrt{T}} = O\left(\frac{1}{\sqrt{T}}\right)$成立。

	~\\
	~\\
	~\\
	~\\	
	~\\
\end{myAnalysis}


\newpage
\section{[50pts] Stochastic Optimization}
\noindent 考虑随机优化问题
\[
\min_{\mathbf{w} \in \mathcal{W}} \ F(\mathbf{w})= \mathbb{E}_{\xi} \left[ f(\mathbf{w}, \xi) \right]
\]
其中目标函数是$\lambda$强凸的，也就是
\begin{equation}\label{eqn:strong}
F(\mathbf{w}) + \langle \nabla F(\mathbf{w}), \mathbf{w}'-\mathbf{w}  \rangle + \frac{\lambda}{2} \|\mathbf{w}'-\mathbf{w}\|_2^2 \leq F(\mathbf{w}'), \ \forall \mathbf{w}, \mathbf{w}' \in \mathcal{W}.
\end{equation}
试分析采用$\eta_t=O(1/[\lambda t])$的随机梯度下降算法的额外风险。
\begin{enumerate}
  \item[(1)] \textbf{[25pts]} 证明期望意义上的额外风险为$O(\log T/T)$。\\
  提示：该问题非常简单，可以将步长设置为$\eta_t=1/[\lambda t]$。然后，参考[1]中定理1的证明，得到
\[
 \sum_{t=1}^T F(\mathbf{w}_{t}) - T F(\mathbf{w}) \leq \frac{G^2}{2 \lambda} (\log T+1) + \sum_{t=1}^T \langle \nabla F(\mathbf{w}_t)-\mathbf{g}_t, \mathbf{w}_t - \mathbf{w} \rangle.
\]
接下来只需要求期望，化简即可。
  \item[(2)] \textbf{[25pts]} 证明$O(\log T/T)$的收敛速率同样以大概率成立。\\
第一种途径：可以将步长设置为$\eta_t=2/[\lambda t])$。然后，参考[1]中定义1的证明，得到
\[
 \sum_{t=1}^T F(\mathbf{w}_{t}) - T F(\mathbf{w}) \leq    \frac{G^2}{\lambda}(\log T+1) +\sum_{t=1}^T \langle \nabla F(\mathbf{w}_t)-\mathbf{g}_t, \mathbf{w}_t - \mathbf{w} \rangle- \frac{\lambda}{4} \sum_{t=1}^T \|\mathbf{w}_t - \mathbf{w}\|^2.
\]
参考讲义公式(21)的推导过程，我们知道以至少$1-\delta$的概率
 \[
 \sum_{t=1}^T \langle \nabla F(\mathbf{w}_t)-\mathbf{g}_t, \mathbf{w}_t - \mathbf{w} \rangle \leq  2\sqrt{ 4 G^2  \log \frac{m }{\delta} \left( \sum_{t=1}^T \|\mathbf{w}_t - \mathbf{w}\|^2\right)} +  \frac{8G^2}{3\lambda}   \log \frac{m }{\delta} + \frac{4G^2}{\lambda} .
\]
最后对结果化简即可。\\
第二种途径：参考论文[2]。
\end{enumerate}
\begin{myProof}~\\
$(1)$设最优值为$\mathbf{w}$，由$\lambda$强凸的性质可以知道：
\begin{eqnarray*}
F(\mathbf{w}_t) - F(\mathbf{w}) &\le& \langle \nabla F(\mathbf{w}_t), \mathbf{w}_t - \mathbf{w} \rangle - \frac{\lambda}{2} \norm{\mathbf{w} - \mathbf{w}_t}_2^2 \\
&=& \langle \nabla f(\mathbf{w}_t, \xi_t), \mathbf{w}_t - \mathbf{w} \rangle + \langle \nabla F(\mathbf{w}_t) - f(\mathbf{w}_t, \xi_t) , \mathbf{w}_t - \mathbf{w} \rangle  - \frac{\lambda}{2} \norm{\mathbf{w} - \mathbf{w}_t}_2^2
\end{eqnarray*}
记$\epsilon_t = \langle \nabla F(\mathbf{w}_t) - f(\mathbf{w}_t, \xi_t) , \mathbf{w}_t - \mathbf{w} \rangle$进而有：
\begin{eqnarray*}
F(\mathbf{w}_t) - F(\mathbf{w})
&\le& \langle \nabla f(\mathbf{w}_t, \xi_t), \mathbf{w}_t - \mathbf{w} \rangle   - \frac{\lambda}{2} \norm{\mathbf{w} - \mathbf{w}_t}_2^2 + \epsilon_t \\
&=& \frac{1}{\eta_t} \langle \mathbf{w}_t - \mathbf{w}_{t+1}', \mathbf{w}_t - \mathbf{w} \rangle - \frac{\lambda}{2} \norm{\mathbf{w} - \mathbf{w}_t}_2^2 + \epsilon_t \\
&=& \frac{1}{2\eta_t} \left[ \norm{\mathbf{w}_t - \mathbf{w}_{t+1}'}_2^2 + \norm{\mathbf{w}_t - \mathbf{w}}_2^2 - \norm{\mathbf{w}_{t+1}' - \mathbf{w}}_2^2 \right] - \frac{\lambda}{2} \norm{\mathbf{w} - \mathbf{w}_t}_2^2 + \epsilon_t \\
&=& \frac{1}{2\eta_t} \left[ \norm{\mathbf{w}_t - \mathbf{w}}_2^2 - \norm{\mathbf{w}_{t+1}' - \mathbf{w}}_2^2 \right] + \frac{1}{2\eta_t} \norm{\mathbf{w}_t - \mathbf{w}_{t+1}'}_2^2 - \frac{\lambda}{2} \norm{\mathbf{w} - \mathbf{w}_t}_2^2 + \epsilon_t \\
&=& \frac{1}{2\eta_t} \left[ \norm{\mathbf{w}_t - \mathbf{w}}_2^2 - \norm{\mathbf{w}_{t+1}' - \mathbf{w}}_2^2 \right] + \frac{\eta_t}{2} \norm{\nabla F(\mathbf{w}_t)}_2^2 - \frac{\lambda}{2} \norm{\mathbf{w} - \mathbf{w}_t}_2^2 + \epsilon_t \\
&\le& \frac{1}{2\eta_t} \left[ \norm{\mathbf{w}_t - \mathbf{w}}_2^2 - \norm{\mathbf{w}_{t+1} - \mathbf{w}}_2^2 \right] + \frac{\eta_t}{2} G^2 - \frac{\lambda}{2} \norm{\mathbf{w} - \mathbf{w}_t}_2^2 + \epsilon_t
\end{eqnarray*}

将上述不等式从$t=1$加到$T$有：
\begin{eqnarray*}
\sum_{t=1}^T F(\mathbf{w}_t) - TF(\mathbf{w}) &\le& \sum_{t=1}^T \left( \frac{1}{2\eta_t} \left[ \norm{\mathbf{w}_t - \mathbf{w}}_2^2 - \norm{\mathbf{w}_{t+1} - \mathbf{w}}_2^2 \right] + \frac{\eta_t}{2} G^2 - \frac{\lambda}{2} \norm{\mathbf{w} - \mathbf{w}_t}_2^2 + \epsilon_t \right) \\
&=& \sum_{t=2}^T \left( \frac{1}{2\eta_t} - \frac{1}{2\eta_{t-1}} - \frac{\lambda}{2} \right) \norm{\mathbf{w}_t - \mathbf{w}}_2^2 + \left( \frac{1}{2\eta_1} - \frac{\lambda}{2} \right) \norm{\mathbf{w}_1 - \mathbf{w}}_2^2 \\
&& - \frac{1}{2\eta_{T+1}}\norm{\mathbf{w}_{T+1} - \mathbf{w}}_2^2 + \sum_{t=1}^T \frac{\eta_t}{2} G^2 + \sum_{t=1}^T \epsilon_t \\
&=&  \sum_{t=2}^T \left( \frac{\lambda t}{2} - \frac{\lambda (t-1)}{2} - \frac{\lambda}{2} \right) \norm{\mathbf{w}_t - \mathbf{w}}_2^2 + \left( \frac{\lambda}{2} - \frac{\lambda}{2} \right) \norm{\mathbf{w}_1 - \mathbf{w}}_2^2 \\
&& - \frac{1}{2\eta_{T+1}}\norm{\mathbf{w}_{T+1} - \mathbf{w}}_2^2 + \sum_{t=1}^T \frac{1}{2\lambda t} G^2 + \sum_{t=1}^T \epsilon_t \\
&\le& \sum_{t=1}^T \frac{1}{2\lambda t} G^2 + \sum_{t=1}^T \epsilon_t < \frac{1 + \ln T}{2\lambda} G^2 + \sum_{t=1}^T \epsilon_t
\end{eqnarray*}

再由Jensen不等式可以得到$F(\overline{\mathbf{w}}_T) - F(\mathbf{w}) \le \frac{1}{T}\sum_{t=1}^T F(\mathbf{w}_t) - F(\mathbf{w}) \le \frac{1 + \ln T}{2\lambda T} G^2 + \frac{1}{T}\sum_{t=1}^T \epsilon_t$，又因为$\mathbb{E}[f(\mathbf{w}_t, \xi_t)] = F(\mathbf{w}_t)$，所以$\mathbb{E}[\epsilon_t] = \mathbb{E}[\langle \nabla F(\mathbf{w}_t) - f(\mathbf{w}_t, \xi_t) , \mathbf{w}_t - \mathbf{w} \rangle] = 0$。最后对上述不等式两边求期望即可得到$\mathbb{E}[F(\overline{\mathbf{w}}_T)] - F(\mathbf{w}) \le \frac{1 + \ln T}{2\lambda T} G^2 = O(\frac{\ln T}{T})$。

$(2)$ 令$\eta_t = \frac{2}{\lambda t}$，则由$(1)$的推导可得
\begin{eqnarray*}
\sum_{t=1}^T F(\mathbf{w}_t) - TF(\mathbf{w}) &\le& \sum_{t=1}^T \left( \frac{1}{2\eta_t} - \frac{1}{2\eta_{t-1}} - \frac{\lambda}{2} \right) \norm{\mathbf{w}_t - \mathbf{w}}_2^2 + \left( \frac{1}{2\eta_1} - \frac{\lambda}{2} \right) \norm{\mathbf{w}_1 - \mathbf{w}}_2^2 \\
&& - \frac{1}{2\eta_{T+1}}\norm{\mathbf{w}_{T+1} - \mathbf{w}}_2^2 + \sum_{t=1}^T \frac{\eta_t}{2} G^2 + \sum_{t=1}^T \epsilon_t \\
&=& \sum_{t=2}^T \left( \frac{\lambda t}{4} - \frac{\lambda (t-1)}{4} - \frac{\lambda}{2} \right) \norm{\mathbf{w}_t - \mathbf{w}}_2^2 + \left( \frac{\lambda}{4} - \frac{\lambda}{2} \right) \norm{\mathbf{w}_1 - \mathbf{w}}_2^2 \\
&& - \frac{1}{2\eta_{T+1}}\norm{\mathbf{w}_{T+1} - \mathbf{w}}_2^2 + \sum_{t=1}^T \frac{1}{\lambda t} G^2 + \sum_{t=1}^T \epsilon_t \\
&\le& \frac{1 + \ln T}{\lambda}G^2 + \sum_{t=1}^T \epsilon_t - \frac{\lambda}{4} \sum_{t=1}^T \norm{\mathbf{w}_t - \mathbf{w}}_2^2 - \frac{1}{2\eta_{T+1}}\norm{\mathbf{w}_{T+1}}_2^2 \\
&\le& \frac{1 + \ln T}{\lambda}G^2 + \sum_{t=1}^T \epsilon_t \\
\Longrightarrow \frac{1}{T}\sum_{t=1}^T F(\mathbf{w}_t) - F(\mathbf{w}) &\le& \frac{1 + \ln T}{\lambda T}G^2 + \frac{1}{T}\sum_{t=1}^T \epsilon_t
\end{eqnarray*}

记$M(\lambda, \frac{1}{\delta}, m) = 2\sqrt{ 4 G^2  \log \frac{m }{\delta} \left( \sum_{t=1}^T \|\mathbf{w}_t - \mathbf{w}\|^2\right)} +  \frac{8G^2}{3\lambda}   \log \frac{m }{\delta} + \frac{4G^2}{\lambda}$，由题目中slides的结论知$\sum_{t=1}^T \epsilon_t \le M(\lambda, \frac{1}{\delta}, m)$以至少$1-\delta$的概率成立，所以$\frac{1}{T}\sum_{t=1}^T F(\mathbf{w}_t) - F(\mathbf{w}) \le \frac{1 + \ln T}{\lambda T}G^2 + \frac{1}{T} M(\lambda, \frac{1}{\delta}, m) = O(\frac{\ln T}{T})$也以至少$1-\delta$的概率成立，再代一步Jensen不等式即可得到$\Pr [ F(\overline{\mathbf{w}}_t) - F(\mathbf{w}) \le \frac{1 + \ln T}{\lambda T}G^2 + \frac{1}{T} M(\lambda, \frac{1}{\delta}, m)] \ge 1 - \delta$，所以$O(\frac{\ln T}{T})$的收敛速率同样以大概率成立。

	~\\	
	\qed
\end{myProof}
\newpage
\section*{Reference}
\begin{enumerate}[ {[}1{]}]
\item E. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for online convex optimization. \textit{Machine Learning}, 69(2-3):169-192, 2007.
\item S. M. Kakade and A. Tewari. On the generalization ability of online strongly convex programming algorithms. In \textit{Advances in Neural Information Processing Systems 21}, pages 801-808, 2009.
\end{enumerate}
\end{document}
